{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inshorts_summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rnrHXGRJd0S9",
        "P-PvCsI1B5Bh",
        "a_b4ou4TYqUN",
        "xluDl5cXYy4y",
        "RdDqGayx67vv",
        "7e7hKcxn6-zd",
        "5rYiMmtGbNjI",
        "p7k4_pi3bRHO",
        "LOCckMYEbTyb",
        "sRsR-DHbbaS-",
        "JY0LARlNbeyg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTe7dYn4dr1R",
        "outputId": "3c434329-1216-4b69-8e09-1f5f116fb89d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3anbrW4Kdlzb"
      },
      "source": [
        "**Link to dataset:**https://drive.google.com/file/d/1Qbm_cd2B4pgROGxmR6TjopvwCuyONcEZ/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnrHXGRJd0S9"
      },
      "source": [
        "## **Data Preparation:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSQr3ny6d69N",
        "outputId": "6a041c44-670e-4120-e829-7027bc39b550"
      },
      "source": [
        "import pandas as pd\n",
        "path = '/content/gdrive/MyDrive/news.xlsx'\n",
        "news = pd.read_excel(path)\n",
        "print('Loaded data samples :', len(news))\n",
        "news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data samples : 55104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "      <td>The New Indian Express</td>\n",
              "      <td>09:25:00</td>\n",
              "      <td>2017-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "      <td>Outlook</td>\n",
              "      <td>22:18:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>23:39:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>23:08:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>23:24:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ... Publish Date\n",
              "0  4 ex-bank officials booked for cheating bank o...  ...   2017-03-26\n",
              "1     Supreme Court to go paperless in 6 months: CJI  ...   2017-03-25\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  ...   2017-03-25\n",
              "3  Why has Reliance been barred from trading in f...  ...   2017-03-25\n",
              "4  Was stopped from entering my own studio at Tim...  ...   2017-03-25\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChatcKxRQmZh",
        "outputId": "84563713-9714-46c4-9823-8415644f2717"
      },
      "source": [
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n",
        "news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline                                              Short\n",
              "0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n",
              "1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n",
              "3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
              "4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O2B7Yg5d9yH"
      },
      "source": [
        "document = news['Short']\n",
        "summary = news['Headline']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl68mvxnQ56P",
        "outputId": "5661f6ea-199d-45e7-b39d-4a3c157bc9ba"
      },
      "source": [
        "document[100], summary[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Indian spinner Ravichandran Ashwin has broken the record for most wickets in a single Test season, taking his 79th this season during the Dharamsala Test against Australia on Saturday. Ashwin went past South African pacer Dale Steyn, who had claimed 78 wickets in 12 Tests in 2007-08. Ashwin has taken seven five-wicket hauls this season in 13 matches.',\n",
              " 'Ashwin breaks record for most wickets in a Test season')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU8QdXfznyzS",
        "outputId": "867790a4-ea01-435d-d200-8c7362e95f22"
      },
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> 4 ex-bank officials booked for cheating b...\n",
              "1    <go> Supreme Court to go paperless in 6 months...\n",
              "2    <go> At least 3 killed, 30 injured in blast in...\n",
              "3    <go> Why has Reliance been barred from trading...\n",
              "4    <go> Was stopped from entering my own studio a...\n",
              "Name: Headline, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eODYfGQ3d9lL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(document, summary, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fiu07YdPd9jb",
        "outputId": "726a4716-0741-4177-f726-451de2d894e7"
      },
      "source": [
        "x_train.shape,x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((44083,), (11021,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI8zBCW5lC5D"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOvNIgqsTWUo"
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?-@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'\n",
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters,oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)\n",
        "inputs = document_tokenizer.texts_to_sequences(x_train)\n",
        "targets = summary_tokenizer.texts_to_sequences(y_train)\n",
        "\n",
        "# To test the tokenizer\n",
        "#summary_tokenizer.texts_to_sequences([\"This is a test\"])\n",
        "# summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrVc1gwtUARI",
        "outputId": "b59f374b-a69f-4ddc-b18f-398bc2d53b0c"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76362, 29661)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ob_tS7FsTHg"
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in x_train])\n",
        "summary_lengths = pd.Series([len(y) for y in y_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSSE4Vt1sTEV",
        "outputId": "e9013c90-5370-4263-ae0c-49d4306d1c4d"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    44083.000000\n",
              "mean       368.108613\n",
              "std         26.243402\n",
              "min        280.000000\n",
              "25%        350.000000\n",
              "50%        369.000000\n",
              "75%        387.000000\n",
              "max        469.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho5HmQhmsTB7",
        "outputId": "e2f6f326-7f81-4cc5-bb00-0540d6796bc0"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    44083.000000\n",
              "mean        63.623460\n",
              "std          7.269837\n",
              "min         20.000000\n",
              "25%         59.000000\n",
              "50%         63.000000\n",
              "75%         69.000000\n",
              "max         96.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXKn4-bwsS-_"
      },
      "source": [
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUYcOJQGsS9L"
      },
      "source": [
        "train_inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "train_targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luHiBt9rd9Vj"
      },
      "source": [
        "train_inputs = tf.cast(train_inputs, dtype=tf.int32)\n",
        "train_targets = tf.cast(train_targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjTAK-ugwhSZ"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWMQrYScwhOm"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "182B0VVSz6Ld",
        "outputId": "b255b8e5-eadc-454a-d974-2ae93384eb27"
      },
      "source": [
        "x_batch, y_batch = next(iter(dataset))\n",
        "x_batch, y_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 400), dtype=int32, numpy=\n",
              " array([[ 152, 1477, 3854, ...,    0,    0,    0],\n",
              "        [   2,  580, 8372, ...,    0,    0,    0],\n",
              "        [ 133,  527, 9449, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 4570,   61, ...,    0,    0,    0],\n",
              "        [ 396, 1955, 1351, ...,    0,    0,    0],\n",
              "        [   7, 3571,   54, ...,    0,    0,    0]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(64, 75), dtype=int32, numpy=\n",
              " array([[   2,   67,  685, ...,    0,    0,    0],\n",
              "        [   2,  432,   56, ...,    0,    0,    0],\n",
              "        [   2,   51,  580, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 2624,  572, ...,    0,    0,    0],\n",
              "        [   2, 1536, 1034, ...,    0,    0,    0],\n",
              "        [   2, 3238,   34, ...,    0,    0,    0]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWFKVgK-whMC",
        "outputId": "d5f900d0-4521-4815-e22f-72d20abe1c14"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 400), (None, 75)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXzVhU34zWEU"
      },
      "source": [
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-PvCsI1B5Bh"
      },
      "source": [
        "## **Positional encoding:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBBMK_eJxfq3"
      },
      "source": [
        "def get_angles(pos, j, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (j//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsqSEQyix4hT"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "# **Masking**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjwnzI0SaNp2"
      },
      "source": [
        "## **1.Padding Mask**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvUF732_qC2"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzS54EOXaWGH"
      },
      "source": [
        "## **2.look ahead Mask**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjTW5QBlA67E"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "# **Scaled dot product attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOoGydi9adu_"
      },
      "source": [
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V} $$\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\n",
        "\n",
        "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95DgD0L-7_yp"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## **Multi-head attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AbDlNs8alsd"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxNpKWeE8H4I"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDqGayx67vv"
      },
      "source": [
        "# **Point wise feed forward network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pErXy7Up8JcW"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "# **Encoder and decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebNmgVJDa1ef"
      },
      "source": [
        "## **Encoder layer**\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NCACx8j8MmP"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wLbYLPRa7FP"
      },
      "source": [
        "## **Decoder layer**\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n1Iac3C8RCR"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwTQzjwtbAtj"
      },
      "source": [
        "## **Encoder**\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6QFZGYj8VQz"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHYJPnmbFDh"
      },
      "source": [
        "## **Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ko4KYtXbJLt"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jfJ1XNZ8ZJJ"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYiMmtGbNjI"
      },
      "source": [
        "# **Transformer**\n",
        "\n",
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n56wEMSr8dlf"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7k4_pi3bRHO"
      },
      "source": [
        "# **Model Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBuVPx618gTl"
      },
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOCckMYEbTyb"
      },
      "source": [
        "# **Optimizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ-qB1oFbWls"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAE-17f8kO0"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_wL_aiB8kJ2"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRsR-DHbbaS-"
      },
      "source": [
        "# **Loss and Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnH7XEGM8q9C"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXZQZoWI8uD5"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw2bkQkA8kIV"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY0LARlNbeyg"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQCb0X128j8J"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYL18BcM8j5Q"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0uwBh783SF"
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMaM8dVabkae"
      },
      "source": [
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "\n",
        "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZqBMKqC83N9"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yQmE8LY9A31",
        "outputId": "cdd3f5c0-7697-40de-96db-a06effb47b54"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 44k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 44k / 64 ~ 687; 687 / 4 = 171\n",
        "        if batch % 171 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.3087\n",
            "Epoch 1 Batch 171 Loss 10.0422\n",
            "Epoch 1 Batch 342 Loss 9.5058\n",
            "Epoch 1 Batch 513 Loss 8.9287\n",
            "Epoch 1 Batch 684 Loss 8.5325\n",
            "Epoch 1 Loss 8.5245\n",
            "Time taken for 1 epoch: 322.90754079818726 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.1064\n",
            "Epoch 2 Batch 171 Loss 7.0782\n",
            "Epoch 2 Batch 342 Loss 6.9943\n",
            "Epoch 2 Batch 513 Loss 6.9158\n",
            "Epoch 2 Batch 684 Loss 6.8456\n",
            "Epoch 2 Loss 6.8438\n",
            "Time taken for 1 epoch: 309.6895275115967 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.5504\n",
            "Epoch 3 Batch 171 Loss 6.3418\n",
            "Epoch 3 Batch 342 Loss 6.2944\n",
            "Epoch 3 Batch 513 Loss 6.2453\n",
            "Epoch 3 Batch 684 Loss 6.2059\n",
            "Epoch 3 Loss 6.2055\n",
            "Time taken for 1 epoch: 309.6541359424591 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.6824\n",
            "Epoch 4 Batch 171 Loss 5.7797\n",
            "Epoch 4 Batch 342 Loss 5.7466\n",
            "Epoch 4 Batch 513 Loss 5.7223\n",
            "Epoch 4 Batch 684 Loss 5.7039\n",
            "Epoch 4 Loss 5.7037\n",
            "Time taken for 1 epoch: 309.4056673049927 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.6160\n",
            "Epoch 5 Batch 171 Loss 5.3208\n",
            "Epoch 5 Batch 342 Loss 5.3161\n",
            "Epoch 5 Batch 513 Loss 5.3054\n",
            "Epoch 5 Batch 684 Loss 5.3021\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 5.3020\n",
            "Time taken for 1 epoch: 310.4280197620392 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.8351\n",
            "Epoch 6 Batch 171 Loss 4.9403\n",
            "Epoch 6 Batch 342 Loss 4.9525\n",
            "Epoch 6 Batch 513 Loss 4.9597\n",
            "Epoch 6 Batch 684 Loss 4.9629\n",
            "Epoch 6 Loss 4.9638\n",
            "Time taken for 1 epoch: 309.55294156074524 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.5715\n",
            "Epoch 7 Batch 171 Loss 4.5800\n",
            "Epoch 7 Batch 342 Loss 4.6003\n",
            "Epoch 7 Batch 513 Loss 4.6042\n",
            "Epoch 7 Batch 684 Loss 4.6161\n",
            "Epoch 7 Loss 4.6160\n",
            "Time taken for 1 epoch: 309.7216966152191 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.0107\n",
            "Epoch 8 Batch 171 Loss 4.2037\n",
            "Epoch 8 Batch 342 Loss 4.2236\n",
            "Epoch 8 Batch 513 Loss 4.2374\n",
            "Epoch 8 Batch 684 Loss 4.2603\n",
            "Epoch 8 Loss 4.2605\n",
            "Time taken for 1 epoch: 309.5598509311676 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.0624\n",
            "Epoch 9 Batch 171 Loss 3.8900\n",
            "Epoch 9 Batch 342 Loss 3.9077\n",
            "Epoch 9 Batch 513 Loss 3.9258\n",
            "Epoch 9 Batch 684 Loss 3.9551\n",
            "Epoch 9 Loss 3.9551\n",
            "Time taken for 1 epoch: 309.55005717277527 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.3757\n",
            "Epoch 10 Batch 171 Loss 3.5837\n",
            "Epoch 10 Batch 342 Loss 3.6208\n",
            "Epoch 10 Batch 513 Loss 3.6589\n",
            "Epoch 10 Batch 684 Loss 3.6962\n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 Loss 3.6971\n",
            "Time taken for 1 epoch: 310.03985381126404 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.2087\n",
            "Epoch 11 Batch 171 Loss 3.3381\n",
            "Epoch 11 Batch 342 Loss 3.3909\n",
            "Epoch 11 Batch 513 Loss 3.4341\n",
            "Epoch 11 Batch 684 Loss 3.4822\n",
            "Epoch 11 Loss 3.4839\n",
            "Time taken for 1 epoch: 309.6049563884735 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.2887\n",
            "Epoch 12 Batch 171 Loss 3.1563\n",
            "Epoch 12 Batch 342 Loss 3.1997\n",
            "Epoch 12 Batch 513 Loss 3.2551\n",
            "Epoch 12 Batch 684 Loss 3.3052\n",
            "Epoch 12 Loss 3.3066\n",
            "Time taken for 1 epoch: 309.37040662765503 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.1545\n",
            "Epoch 13 Batch 171 Loss 2.9903\n",
            "Epoch 13 Batch 342 Loss 3.0499\n",
            "Epoch 13 Batch 513 Loss 3.0973\n",
            "Epoch 13 Batch 684 Loss 3.1501\n",
            "Epoch 13 Loss 3.1510\n",
            "Time taken for 1 epoch: 309.3978650569916 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.8893\n",
            "Epoch 14 Batch 171 Loss 2.8373\n",
            "Epoch 14 Batch 342 Loss 2.9071\n",
            "Epoch 14 Batch 513 Loss 2.9484\n",
            "Epoch 14 Batch 684 Loss 3.0056\n",
            "Epoch 14 Loss 3.0072\n",
            "Time taken for 1 epoch: 308.79836893081665 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.5250\n",
            "Epoch 15 Batch 171 Loss 2.7008\n",
            "Epoch 15 Batch 342 Loss 2.7607\n",
            "Epoch 15 Batch 513 Loss 2.8082\n",
            "Epoch 15 Batch 684 Loss 2.8651\n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
            "Epoch 15 Loss 2.8665\n",
            "Time taken for 1 epoch: 308.7688682079315 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.4781\n",
            "Epoch 16 Batch 171 Loss 2.5798\n",
            "Epoch 16 Batch 342 Loss 2.6341\n",
            "Epoch 16 Batch 513 Loss 2.6829\n",
            "Epoch 16 Batch 684 Loss 2.7402\n",
            "Epoch 16 Loss 2.7406\n",
            "Time taken for 1 epoch: 308.27389907836914 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.3665\n",
            "Epoch 17 Batch 171 Loss 2.4486\n",
            "Epoch 17 Batch 342 Loss 2.5089\n",
            "Epoch 17 Batch 513 Loss 2.5618\n",
            "Epoch 17 Batch 684 Loss 2.6216\n",
            "Epoch 17 Loss 2.6230\n",
            "Time taken for 1 epoch: 308.2048876285553 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.4795\n",
            "Epoch 18 Batch 171 Loss 2.3312\n",
            "Epoch 18 Batch 342 Loss 2.4002\n",
            "Epoch 18 Batch 513 Loss 2.4531\n",
            "Epoch 18 Batch 684 Loss 2.5170\n",
            "Epoch 18 Loss 2.5183\n",
            "Time taken for 1 epoch: 308.331503868103 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.2780\n",
            "Epoch 19 Batch 171 Loss 2.2418\n",
            "Epoch 19 Batch 342 Loss 2.3010\n",
            "Epoch 19 Batch 513 Loss 2.3504\n",
            "Epoch 19 Batch 684 Loss 2.4131\n",
            "Epoch 19 Loss 2.4151\n",
            "Time taken for 1 epoch: 307.9770510196686 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.0915\n",
            "Epoch 20 Batch 171 Loss 2.1496\n",
            "Epoch 20 Batch 342 Loss 2.2113\n",
            "Epoch 20 Batch 513 Loss 2.2616\n",
            "Epoch 20 Batch 684 Loss 2.3230\n",
            "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
            "Epoch 20 Loss 2.3247\n",
            "Time taken for 1 epoch: 308.7152171134949 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax8K4lhKbuKJ"
      },
      "source": [
        "# **Evaluate**\n",
        "\n",
        "The following steps are used for evaluation:\n",
        "\n",
        "* Encode the input sentence using the Portuguese tokenizer (`tokenizers.pt`). This is the encoder input.\n",
        "* The decoder input is initialized to the `[START]` token.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* The model makes predictions of the next word for each word in the output. Most of these are redundant. Use the predictions from the last word.\n",
        "* Concatenate the predicted word to the decoder input and pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next word based on the previous words it predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4gYmQ29FUP"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWC7ImBZ83Lw"
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPtNVak8VJiR",
        "outputId": "d24d5260-3dd1-4359-f882-504514555fc3"
      },
      "source": [
        "x_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2098     Gangster-turned-politician Mukhtar Ansari has ...\n",
              "8760     Indira Gandhi has been the only woman till dat...\n",
              "42126    Actor Adam Sandler brought his 23-year-old dop...\n",
              "28632    The Supreme Court recently reminded the Centre...\n",
              "36240    Researchers at the University of Stuttgart hav...\n",
              "Name: Short, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM7FpWut9Q2f",
        "outputId": "19b93703-3362-4cd7-d554-8baaf64bc5eb"
      },
      "source": [
        "summary = summarize(x_test[28632])\n",
        "print('original text:',x_test[28632])\n",
        "print('predicted summary:',summary)\n",
        "print('True summary:',y_test[28632])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text: The Supreme Court recently reminded the Centre that Aadhaar cannot be made mandatory for any services. The apex court also ordered the Centre to remove its condition of making Aadhaar mandatory in scholarship schemes for students. &#34;The Aadhaar card Scheme is purely voluntary and cannot be made mandatory till the matter is finally decided by this Court,&#34; the SC added.\n",
            "predicted summary: aadhaar card to tackle a minor cuts\n",
            "True summary: <go> Aadhaar cannot be mandatory, SC reminds govt <stop>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbjUha98Iobn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62477d5-09ce-4b6c-8fc8-c78f5d3b84f1"
      },
      "source": [
        "summary = summarize(x_test[2098])\n",
        "print('original text:',x_test[2098])\n",
        "print('predicted summary:',summary)\n",
        "print('True summary:',y_test[2098])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text: Gangster-turned-politician Mukhtar Ansari has won from the Mau constituency in Uttar Pradesh after polling 96,793 votes, defeating the nearest candidate by over 8,000 votes. Ansari, who was the sitting MLA from the constituency, had allied with the Mayawati-led Bahujan Samaj Party before the elections. Ansari has been accused of murdering a BJP MLA.\n",
            "predicted summary: former miss universe passes away at 72 000 yr old\n",
            "True summary: <go> Gangster-turned-politician Mukhtar Ansari wins by 8000 votes <stop>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzQX4chX83Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6021f754-6e01-4b7d-f8d4-53c043d52c16"
      },
      "source": [
        "summary = summarize(x_test[8760])\n",
        "print('original text:',x_test[8760])\n",
        "print('predicted summary:',summary)\n",
        "print('True summary:',y_test[8760])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text: Indira Gandhi has been the only woman till date to have presented the Union Budget of India in 1970-71. This came after Indira Gandhi, the then Prime Minister, took over the Finance portfolio after Morarji Desai resigned as the Minister of Finance. So far, she has been the only woman Finance Minister of India.\n",
            "predicted summary: i t dept detects demonetisation in india 39 s dream\n",
            "True summary: <go> Indira Gandhi only woman to have presented the budget <stop>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmRw2-YY83Go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a700a65e-fdc3-4656-8d31-4b568015d8d3"
      },
      "source": [
        "summary = summarize(x_test[36240])\n",
        "print('original text:',x_test[36240])\n",
        "print('predicted summary:',summary)\n",
        "print('True summary:',y_test[36240])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text: Researchers at the University of Stuttgart have built wall-climbing mini robots that work together to create architecture from carbon fibre. The robots carry carbon fibre thread spools that they pass back and forth after affixing to points on a wall. The researchers are planning to increase the number of robots, allowing them to attach fibre to ceilings and curved walls.\n",
            "predicted summary: california could install tree near lights\n",
            "True summary: <go> Robots create architecture with carbon fibre <stop>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmInbLqP3x2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}